{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11373757",
   "metadata": {},
   "source": [
    "# 미션: 글로벌 스포츠 이벤트 실시간 트위터 감정 모니터링\n",
    "\n",
    "당신은 세계적인 스포츠 대회 조직위원회 SNS 분석팀의 신규 AI 엔지니어입니다. 대회 기간 중 팬들이 트위터에 올리는 다양한 반응을 실시간으로 분석해, 긍정·부정 감정을 파악하고 주요 이슈를 빠르게 리포트해야 합니다. 이를 위해 아래 과제를 수행하세요.\n",
    "\n",
    "---\n",
    "**과제: SNS 감정 분석 파이프라인 구축**\n",
    "\n",
    "다음 단계별 과제를 Python(Jupyter Notebook)으로 구현하여 제출하세요. 각 단계별로 코드와 간단한 설명(markdown)을 함께 달아야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a3e77",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드\n",
    "- `nltk.download('twitter_samples')`로 리소스 설치\n",
    "- 긍정 트윗(`positive_tweets.json`) 1,000개, 부정 트윗(`negative_tweets.json`) 1,000개 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b02f8679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\wjdgn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 단계 코드 작성\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb77384",
   "metadata": {},
   "source": [
    "## 2. 텍스트 전처리 (Cleaning & Normalization)\n",
    "- 소문자화: 모든 문자를 소문자로 변환\n",
    "- 불필요 문자 제거: URL, 이모지, 특수문자, 숫자 등을 정규표현식으로 제거\n",
    "- 공백 정리: 연속된 공백을 하나로 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "42a7b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "After : for being top engaged members in my community this week\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 2. 텍스트 전처리 (Cleaning & Normalization) 단계 코드 작성\n",
    "def clean_tweet(text):\n",
    "    # 소문자화\n",
    "    text = text.lower()\n",
    "    # URL 제거\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # 이모지 및 특수문자 제거 (유니코드 범위)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "    # 특수문자, 숫자, 언급, 해시태그, &amp; 등 제거\n",
    "    text = re.sub(r'[@#]\\w+|&\\w+;|[^a-z\\s]', ' ', text)\n",
    "    # 숫자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # 다중 공백을 단일 공백으로\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 긍정/부정 트윗 전처리\n",
    "pos_clean = [clean_tweet(t) for t in pos_tweets]\n",
    "neg_clean = [clean_tweet(t) for t in neg_tweets]\n",
    "\n",
    "# 예시 출력\n",
    "print(\"Before:\", pos_tweets[0])\n",
    "print(\"After :\", pos_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9ba3b",
   "metadata": {},
   "source": [
    "## 3. 토큰화 (Sentence & Word Tokenization)\n",
    "- `sent_tokenize`로 샘플 트윗 5개를 문장 단위로 분리\n",
    "- `word_tokenize`로 각 문장을 단어 토큰화하여 첫 10개 토큰 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfa9eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1번 트윗]\n",
      "문장 분리: ['for being top engaged members in my community this week']\n",
      "  문장 1 토큰(최대 10개): ['for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week']\n",
      "\n",
      "[2번 트윗]\n",
      "문장 분리: ['hey james how odd please call our contact centre on and we will be able to assist you many thanks']\n",
      "  문장 1 토큰(최대 10개): ['hey', 'james', 'how', 'odd', 'please', 'call', 'our', 'contact', 'centre', 'on']\n",
      "\n",
      "[3번 트윗]\n",
      "문장 분리: ['we had a listen last night as you bleed is an amazing track when are you in scotland']\n",
      "  문장 1 토큰(최대 10개): ['we', 'had', 'a', 'listen', 'last', 'night', 'as', 'you', 'bleed', 'is']\n",
      "\n",
      "[4번 트윗]\n",
      "문장 분리: ['congrats']\n",
      "  문장 1 토큰(최대 10개): ['congrats']\n",
      "\n",
      "[5번 트윗]\n",
      "문장 분리: ['yeaaaah yippppy my accnt verified rqst has succeed got a blue tick mark on my fb profile in days']\n",
      "  문장 1 토큰(최대 10개): ['yeaaaah', 'yippppy', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wjdgn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 3. 토큰화 (Sentence & Word Tokenization) 단계 코드 작성\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 샘플 트윗 5개 선택 (전처리 전 원문 사용)\n",
    "sample_tweets = pos_clean[:5]\n",
    "\n",
    "# 각 트윗을 문장 단위로 분리\n",
    "for i, tweet in enumerate(sample_tweets):\n",
    "    print(f\"\\n[{i+1}번 트윗]\")\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    print(\"문장 분리:\", sentences)\n",
    "    # 각 문장을 단어 토큰화하여 첫 10개 토큰 출력\n",
    "    for j, sent in enumerate(sentences):\n",
    "        tokens = word_tokenize(sent)\n",
    "        print(f\"  문장 {j+1} 토큰(최대 10개):\", tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10f5c4",
   "metadata": {},
   "source": [
    "## 4. 불용어 제거 (Stopwords)\n",
    "- NLTK 영어 불용어 목록(`stopwords.words('english')`) 적용\n",
    "- 추가로 직접 정의한 ‘비속어·의미 없는 단어’ 5개 포함하여 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40afa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wjdgn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "긍정 트윗 불용어 제거 예시:\n",
      "원문: for being top engaged members in my community this week\n",
      "불용어 제거: ['top', 'engaged', 'members', 'community', 'week']\n",
      "원문: hey james how odd please call our contact centre on and we will be able to assist you many thanks\n",
      "불용어 제거: ['hey', 'james', 'odd', 'please', 'call', 'contact', 'centre', 'able', 'assist', 'many', 'thanks']\n",
      "원문: we had a listen last night as you bleed is an amazing track when are you in scotland\n",
      "불용어 제거: ['listen', 'last', 'night', 'bleed', 'amazing', 'track', 'scotland']\n",
      "\n",
      "긍정 트윗 커스터 불용어 제거 예시:\n",
      "원문: for being top engaged members in my community this week\n",
      "불용어 제거: ['engaged', 'members', 'community', 'week']\n",
      "원문: hey james how odd please call our contact centre on and we will be able to assist you many thanks\n",
      "불용어 제거: ['hey', 'james', 'odd', 'please', 'call', 'contact', 'centre', 'able', 'assist', 'many', 'thanks']\n",
      "원문: we had a listen last night as you bleed is an amazing track when are you in scotland\n",
      "불용어 제거: ['night', 'bleed', 'amazing', 'track', 'scotland']\n"
     ]
    }
   ],
   "source": [
    "# 4. 불용어 제거 (Stopwords) 단계 코드 작성\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# wonna gonna 를 want to , going to 로 변환\n",
    "def replace_wanna_gonna(text):\n",
    "    text = re.sub(r'\\bwanna\\b', 'want to', text)\n",
    "    text = re.sub(r'\\bgonna\\b', 'going to', text)\n",
    "    return text\n",
    "# 긍정/부정 트윗에서 wanna, gonna 변환\n",
    "pos_clean = [replace_wanna_gonna(tweet) for tweet in pos_clean]\n",
    "neg_clean = [replace_wanna_gonna(tweet) for tweet in neg_clean]\n",
    "# 불용어 제거 함수\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "# 긍정/부정 트윗에서 불용어 제거\n",
    "pos_tokens = [word_tokenize(tweet) for tweet in pos_clean]\n",
    "neg_tokens = [word_tokenize(tweet) for tweet in neg_clean]\n",
    "pos_tokens = [remove_stopwords(tokens) for tokens in pos_tokens]\n",
    "neg_tokens = [remove_stopwords(tokens) for tokens in neg_tokens]\n",
    "# 예시 출력\n",
    "print(\"\\n긍정 트윗 불용어 제거 예시:\")\n",
    "for i in range(3):\n",
    "    print(f\"원문: {pos_clean[i]}\")\n",
    "    print(\"불용어 제거:\", pos_tokens[i])\n",
    "\n",
    "custom_stopwords = ['top', 'listen', 'last']\n",
    "# 사용자 정의 불용어 추가\n",
    "stop_words.update(custom_stopwords)\n",
    "# 사용자 정의 불용어 제거\n",
    "def remove_custom_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "# 긍정/부정 트윗에서 사용자 정의 불용어 제거\n",
    "pos_tokens = [remove_custom_stopwords(tokens) for tokens in pos_tokens]\n",
    "neg_tokens = [remove_custom_stopwords(tokens) for tokens in neg_tokens]\n",
    "# 예시 출력\n",
    "print(\"\\n긍정 트윗 커스터 불용어 제거 예시:\")\n",
    "for i in range(3):\n",
    "    print(f\"원문: {pos_clean[i]}\")\n",
    "    print(\"불용어 제거:\", pos_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118feb99",
   "metadata": {},
   "source": [
    "## 5. 단어 사전 구축 (Vocabulary)\n",
    "- 전처리된 전체 토큰에서 단어 빈도수 계산\n",
    "- 상위 5,000개 단어에 고유 인덱스 부여하여 사전 생성\n",
    "- 사전 크기, 상위 20개 단어 및 각 단어의 빈도를 표로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d65c78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "상위 20개 단어 및 빈도수:\n",
      "thanks: 470\n",
      "follow: 446\n",
      "u: 441\n",
      "like: 425\n",
      "want: 416\n",
      "love: 405\n",
      "please: 370\n",
      "get: 349\n",
      "good: 334\n",
      "day: 308\n",
      "back: 283\n",
      "know: 278\n",
      "thank: 277\n",
      "see: 275\n",
      "one: 272\n",
      "miss: 259\n",
      "time: 254\n",
      "going: 228\n",
      "much: 228\n",
      "today: 226\n"
     ]
    }
   ],
   "source": [
    "# 5. 단어 사전 구축 (Vocabulary) 단계 코드 작성\n",
    "from collections import Counter\n",
    "# 전체 트윗에서 단어 사전 구축\n",
    "all_tweets = pos_tokens + neg_tokens\n",
    "\n",
    "# 상위 5000개 단어에 고유 인덱스 부여하여 사전 구축축\n",
    "vocab = Counter(word for tokens in all_tweets for word in tokens)\n",
    "vocab = vocab.most_common(5000)\n",
    "# 단어 빈도수 계산\n",
    "word_freq = Counter(word for tokens in all_tweets for word in tokens)\n",
    "# 상위 20개 단어 출력\n",
    "print(\"\\n상위 20개 단어 및 빈도수:\")\n",
    "for word, freq in word_freq.most_common(20):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f9262",
   "metadata": {},
   "source": [
    "## 6. 정수 인코딩 & 패딩 (Integer Encoding & Padding)\n",
    "- 각 트윗을 사전 인덱스로 변환하여 정수 시퀀스 생성\n",
    "- 최대 길이 50, `padding='post'` 방식으로 패딩\n",
    "- 패딩 전·후 한 문장 예시 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d7845df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (0.0.9)\n",
      "Requirement already satisfied: h5py in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wjdgn\\appdata\\roaming\\python\\python312\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\wjdgn\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wjdgn\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wjdgn\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\wjdgn\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\wjdgn\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e076277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 정수 인코딩 & 패딩 (Integer Encoding & Padding) 단계 코드 작성\n",
    "# 각 트윗을 사전 인덱스로 변환하여 정수 시퀀스 생성\n",
    "word_to_index = {word: i for i, (word, _) in enumerate(vocab)}\n",
    "def encode_tweet(tokens):\n",
    "    return [word_to_index[word] for word in tokens if word in word_to_index]\n",
    "# 정수 인코딩\n",
    "pos_encoded = [encode_tweet(tokens) for tokens in pos_tokens]\n",
    "neg_encoded = [encode_tweet(tokens) for tokens in neg_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "daa1987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "패딩 전/후 문장 예시:\n",
      "원문: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "전처리 후: for being top engaged members in my community this week\n",
      "패딩 전: [1011, 497, 270, 51]\n",
      "패딩 후: [1011  497  270   51    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "원문: @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "전처리 후: hey james how odd please call our contact centre on and we will be able to assist you many thanks\n",
      "패딩 전: [59, 615, 1974, 6, 183, 814, 1975, 271, 4088, 120, 0]\n",
      "패딩 후: [  59  615 1974    6  183  814 1975  271 4088  120    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "원문: @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "전처리 후: we had a listen last night as you bleed is an amazing track when are you in scotland\n",
      "패딩 전: [54, 2601, 110, 1154, 1559]\n",
      "패딩 후: [  54 2601  110 1154 1559    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이 50, padding = 'post' 방식으로 패딩\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 50\n",
    "pos_padded = pad_sequences(pos_encoded, maxlen=max_length, padding='post')\n",
    "neg_padded = pad_sequences(neg_encoded, maxlen=max_length, padding='post')\n",
    "\n",
    "# 패딩 전.후 문장 예시 출력\n",
    "print(\"\\n패딩 전/후 문장 예시:\")\n",
    "for i in range(3):\n",
    "    print(f\"원문: {pos_tweets[i]}\")\n",
    "    print(\"전처리 후:\", pos_clean[i])\n",
    "    print(\"패딩 전:\", pos_encoded[i])\n",
    "    print(\"패딩 후:\", pos_padded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480277b",
   "metadata": {},
   "source": [
    "## 7. 벡터화 (Vectorization)\n",
    "- Bag-of-Words 및 TF–IDF 방식 적용\n",
    "- 전체 코퍼스에 대해 문서–단어 희소 행렬 생성\n",
    "- 희소 행렬 형태와 밀집 배열(첫 5개 문서) 비교 출력\n",
    "- `sklearn.feature_extraction.text` 모듈 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "39517ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words 희소 행렬 형태: (10000, 20) <class 'scipy.sparse._csr.csr_matrix'>\n",
      "TF-IDF 희소 행렬 형태: (10000, 20) <class 'scipy.sparse._csr.csr_matrix'>\n",
      "\n",
      "Bag-of-Words (첫 5개 문서, 밀집 배열):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "TF-IDF (첫 5개 문서, 밀집 배열):\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.73662503 0.         0.         0.67630138 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 1. 토큰 리스트를 다시 문자열로 결합\n",
    "docs = [' '.join(tokens) for tokens in all_tweets]\n",
    "\n",
    "# 2. Bag-of-Words 벡터화\n",
    "bow_vectorizer = CountVectorizer(max_features=20)\n",
    "X_bow = bow_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 3. TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "# 4. 출력\n",
    "print(\"Bag-of-Words 희소 행렬 형태:\", X_bow.shape, type(X_bow))\n",
    "print(\"TF-IDF 희소 행렬 형태:\", X_tfidf.shape, type(X_tfidf))\n",
    "\n",
    "print(\"\\nBag-of-Words (첫 5개 문서, 밀집 배열):\")\n",
    "print(X_bow[:5].toarray())\n",
    "\n",
    "print(\"\\nTF-IDF (첫 5개 문서, 밀집 배열):\")\n",
    "print(X_tfidf[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7d3f84d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "벡터화된 단어 수: 20\n",
      "상위 단어 10개: ['please', 'thanks', 'one', 'like', 'time', 'go', 'love', 'follow', 'back', 'know']\n"
     ]
    }
   ],
   "source": [
    "# 벡터화 후 실제 단어 확인\n",
    "print(\"\\n벡터화된 단어 수:\", len(bow_vectorizer.vocabulary_))\n",
    "print(\"상위 단어 10개:\", list(bow_vectorizer.vocabulary_.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624f637",
   "metadata": {},
   "source": [
    "## 8. Bigram 모델 (간단 예제)\n",
    "아래 아주 단순한 코퍼스로 Bigram 확률을 직접 계산해 보세요:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "78ea59a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('학교'|'나는') = 0.0, P('집'|'나는') = 0.00\n"
     ]
    }
   ],
   "source": [
    "# 8. Bigram 모델 (간단 예제) 단계 코드 작성\n",
    "# 이 코드는 전처리가 제대로 안되어 실패하는 코드임\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "corpus = [\"나는 학교에 간다\", \"나는 집에 간다\", \"나는 학교에 간다\"]\n",
    "tokenized = [s.split() for s in corpus]\n",
    "\n",
    "unigram = Counter()\n",
    "bigram  = Counter()\n",
    "for toks in tokenized:\n",
    "    unigram.update(toks)\n",
    "    bigram.update(zip(toks[:-1], toks[1:]))\n",
    "\n",
    "prev = \"나는\"\n",
    "p_school = bigram[(prev,\"학교\")] / unigram[prev]\n",
    "p_home   = bigram[(prev,\"집\")]   / unigram[prev]\n",
    "print(f\"P('학교'|'나는') = {p_school}, P('집'|'나는') = {p_home:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0127e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과: [['나', '학교', '간다'], ['나', '집', '간다'], ['나', '학교', '간다']]\n",
      "P('학교'|'나') = 0.67, P('집'|'나') = 0.33\n"
     ]
    }
   ],
   "source": [
    "# 8. Bigram 모델 (간단 예제) 단계 코드 작성\n",
    "\n",
    "# konlpy 설치가 필요하다면\n",
    "# !pip install konlpy\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 1) 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 2) 원문 코퍼스\n",
    "corpus = [\"나는 학교에 간다\", \"나는 집에 간다\", \"나는 학교에 간다\"]\n",
    "\n",
    "# 3) 형태소 분석 및 불필요 품사 제거\n",
    "tokenized = []\n",
    "for sent in corpus:\n",
    "    pos = okt.pos(sent, norm=True, stem=True)\n",
    "    # Josa(조사), Eomi(어미), Punctuation(구두점) 제외\n",
    "    toks = [word for word, tag in pos if tag not in ('Josa','Eomi','Punctuation')]\n",
    "    tokenized.append(toks)\n",
    "\n",
    "print(\"토큰화 결과:\", tokenized)\n",
    "# → [['나','학교','간다'], ['나','집','간다'], ['나','학교','간다']]\n",
    "\n",
    "# 4) Unigram / Bigram 카운트\n",
    "unigram = Counter()\n",
    "bigram  = Counter()\n",
    "for toks in tokenized:\n",
    "    unigram.update(toks)\n",
    "    bigram.update(zip(toks[:-1], toks[1:]))\n",
    "\n",
    "# 5) 조건부 확률 계산\n",
    "prev = '나'\n",
    "p_school = bigram[(prev, '학교')] / unigram[prev]\n",
    "p_home   = bigram[(prev, '집')]   / unigram[prev]\n",
    "\n",
    "print(f\"P('학교'|'나') = {p_school:.2f}, P('집'|'나') = {p_home:.2f}\")\n",
    "# → P('학교'|'나') = 0.67, P('집'|'나') = 0.33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ac5c7",
   "metadata": {},
   "source": [
    "## 9. 트위터 데이터셋을 활용한 Bigram 모델 실습\n",
    "NLTK `twitter_samples`의 긍정·부정 토큰을 이용해 다음을 수행하세요:\n",
    "\n",
    "- **Unigram / Bigram 카운트**\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "pos_tokens = [...]  # 4번 단계 결과 사용\n",
    "neg_tokens = [...]  # 4번 단계 결과 사용\n",
    "\n",
    "unigram_pos = Counter(pos_tokens)\n",
    "bigram_pos  = Counter(zip(pos_tokens[:-1], pos_tokens[1:]))\n",
    "unigram_neg = Counter(neg_tokens)\n",
    "bigram_neg  = Counter(zip(neg_tokens[:-1], neg_tokens[1:]))\n",
    "```\n",
    "\n",
    "- **조건부 확률 계산**\n",
    "\n",
    "```python\n",
    "p_pos = bigram_pos[('i','love')] / unigram_pos['i']\n",
    "p_neg = bigram_neg[('i','love')] / unigram_neg['i']\n",
    "print(f\"P(i→love|pos) = {p_pos:.2f}, P(i→love|neg) = {p_neg:.2f}\")\n",
    "```\n",
    "\n",
    "- **상위 10개 Bigram 비교**\n",
    "\n",
    "```python\n",
    "print(\"Top10 positive:\", bigram_pos.most_common(10))\n",
    "print(\"Top10 negative:\", bigram_neg.most_common(10))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "465252de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(i→love|pos) = 0.00, P(i→love|neg) = 0.00\n",
      "Top10 positive: [(('follow', 'follow'), 64), (('follow', 'u'), 62), (('u', 'back'), 62), (('arrived', 'new'), 60), (('unfollowers', 'via'), 60), (('happy', 'friday'), 50), (('happy', 'birthday'), 50), (('love', 'x'), 49), (('lot', 'see'), 45), (('hi', 'bam'), 44)]\n",
      "Top10 negative: [(('follow', 'please'), 73), (('thanks', 'please'), 52), (('please', 'followed'), 52), (('followed', 'thanks'), 51), (('please', 'follow'), 45), (('love', 'much'), 43), (('want', 'go'), 42), (('much', 'beli'), 35), (('beli', 'eve'), 35), (('eve', 'wi'), 35)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Unigram / Bigram 카운트\n",
    "# 모든 토큰을 하나의 리스트로 합침\n",
    "flat_pos = [token for tweet in pos_tokens for token in tweet]\n",
    "flat_neg = [token for tweet in neg_tokens for token in tweet]\n",
    "\n",
    "unigram_pos = Counter(flat_pos)\n",
    "unigram_neg = Counter(flat_neg)\n",
    "bigram_pos = Counter(zip(flat_pos[:-1], flat_pos[1:]))\n",
    "bigram_neg = Counter(zip(flat_neg[:-1], flat_neg[1:]))\n",
    "\n",
    "# 2. 조건부 확률 계산\n",
    "p_pos = bigram_pos[('i', 'love')] / unigram_pos['i'] if unigram_pos['i'] > 0 else 0\n",
    "p_neg = bigram_neg[('i', 'love')] / unigram_neg['i'] if unigram_neg['i'] > 0 else 0\n",
    "print(f\"P(i→love|pos) = {p_pos:.2f}, P(i→love|neg) = {p_neg:.2f}\")\n",
    "\n",
    "# 3. 상위 10개 Bigram 비교\n",
    "print(\"Top10 positive:\", bigram_pos.most_common(10))\n",
    "print(\"Top10 negative:\", bigram_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2fc01597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace smoothing 적용 후 P(i→love|pos) = 0.0002, P(i→love|neg) = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Laplace 스무딩(α=1) 적용된 Bigram 조건부 확률 계산\n",
    "\n",
    "# α 설정\n",
    "alpha = 1\n",
    "\n",
    "# 어휘 크기 계산 (positive, negative 각각)\n",
    "V_pos = len(unigram_pos)\n",
    "V_neg = len(unigram_neg)\n",
    "\n",
    "# 스무딩 적용된 조건부 확률\n",
    "p_pos = (bigram_pos[('i', 'love')] + alpha) / (unigram_pos['i'] + alpha * V_pos)\n",
    "p_neg = (bigram_neg[('i', 'love')] + alpha) / (unigram_neg['i'] + alpha * V_neg)\n",
    "\n",
    "print(f\"Laplace smoothing 적용 후 P(i→love|pos) = {p_pos:.4f}, P(i→love|neg) = {p_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57a91006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10 positive: [(('follow', 'follow'), 64), (('follow', 'u'), 62), (('u', 'back'), 62), (('arrived', 'new'), 60), (('unfollowers', 'via'), 60), (('happy', 'friday'), 50), (('happy', 'birthday'), 50), (('love', 'x'), 49), (('lot', 'see'), 45), (('hi', 'bam'), 44)]\n",
      "Top10 negative: [(('follow', 'please'), 73), (('thanks', 'please'), 52), (('please', 'followed'), 52), (('followed', 'thanks'), 51), (('please', 'follow'), 45), (('love', 'much'), 43), (('want', 'go'), 42), (('much', 'beli'), 35), (('beli', 'eve'), 35), (('eve', 'wi'), 35)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top10 positive:\", bigram_pos.most_common(10))\n",
    "print(\"Top10 negative:\", bigram_neg.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00300dad",
   "metadata": {},
   "source": [
    "---\n",
    "## 제출 형식\n",
    "\n",
    "- 각 단계별 코드와 결과, 간단한 설명(markdown) 포함  \n",
    "- 필요에 따라 시각화(matplotlib) 또는 표 활용  \n",
    "- 최종 파일: Jupyter Notebook(.ipynb)으로 제출  \n",
    "\n",
    "이 과제를 통해, 감정 분석의 전체 워크플로우(데이터 로드 → 전처리 → 토큰화/불용어 제거 → 사전 구축 → 인코딩·패딩 → 벡터화 → Bigram 모델 → 감정 분류)를 직접 구현·체험할 수 있습니다. 재미있게 도전해 보세요!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
