{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf9a74",
   "metadata": {},
   "source": [
    "## 도서 리뷰 장르 분류 AI 미션\n",
    "\n",
    "### 배경 스토리  \n",
    "여러분은 온라인 도서 리뷰 플랫폼 **‘북톡(BOOK-Talk)’**의 AI 엔지니어 신입입니다.  \n",
    "사용자가 작성한 리뷰를 분석하여 다음 세 가지 장르 중 하나로 자동 분류하는 시스템을 만들어야 합니다.  \n",
    "- **Novel(소설)**  \n",
    "- **Self-help(자기계발)**  \n",
    "- **History(역사)**  \n",
    "\n",
    "이를 위해 FastText 기반 단어 임베딩과, 네 가지 신경망 모델(**Vanilla FC**, **RNN**, **LSTM**, **GRU**)을 직접 구현하고 성능을 비교해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50becefd",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드: \n",
    "\n",
    "받아서 이름 변경해야.\n",
    "\n",
    "https://www.kaggle.com/datasets/athu1105/book-genre-prediction?utm_source=chatgpt.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34949f",
   "metadata": {},
   "source": [
    "### ✏️ 기초 코드 설명\n",
    "데이터 로드\n",
    "\n",
    "book_reviews.csv에서 리뷰(review)와 장르(genre)를 불러옵니다.\n",
    "\n",
    "전처리\n",
    "\n",
    "영어는 소문자화, 한글·영어·숫자·공백만 남겨 토큰화합니다.\n",
    "\n",
    "FastText 임베딩\n",
    "\n",
    "gensim.downloader로 사전 학습된 FastText 모델(fasttext-wiki-news-subwords-300)을 로드합니다.\n",
    "\n",
    "어휘 사전 구축\n",
    "\n",
    "전체 토큰 빈도 상위 9,999개를 선별해 word2idx 매핑을 만들고, 나머지는 OOV(0) 처리합니다.\n",
    "\n",
    "최대 길이 100으로 패딩·절단해 시퀀스로 변환합니다.\n",
    "\n",
    "PyTorch 준비\n",
    "\n",
    "TensorDataset과 DataLoader로 배치 처리 환경을 구성합니다.\n",
    "\n",
    "임베딩 레이어 초기화\n",
    "\n",
    "FastText 벡터로 nn.Embedding 가중치를 채우고, 미세조정은 하지 않도록 고정(requires_grad=False)했습니다.\n",
    "\n",
    "모델 정의\n",
    "\n",
    "Vanilla FC: 평균 임베딩 후 완전 연결층으로 분류\n",
    "\n",
    "RNN/LSTM/GRU: 각 시퀀스를 순차 처리하고 마지막 hidden state를 분류기로 연결\n",
    "\n",
    "학습·평가\n",
    "\n",
    "Adam 옵티마이저, CrossEntropyLoss로 5 epoch 동안 학습\n",
    "\n",
    "테스트 세트 정확도를 비교\n",
    "\n",
    "결과 해석\n",
    "\n",
    "\n",
    "이 미션을 통해 FastText 단어 임베딩과 다양한 순환 신경망 구조를 직접 구현·비교해 보세요. 반복 학습을 통해 하이퍼파라미터나 모델 구조를 변형해 보는 과제도 추천합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60968ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4657, 4)\n",
      "   index                      title    genre  \\\n",
      "0      0          Drowned Wednesday  fantasy   \n",
      "1      1              The Lost Hero  fantasy   \n",
      "2      2  The Eyes of the Overworld  fantasy   \n",
      "3      3            Magic's Promise  fantasy   \n",
      "4      4             Taran Wanderer  fantasy   \n",
      "\n",
      "                                              review  \n",
      "0   Drowned Wednesday is the first Trustee among ...  \n",
      "1   As the book opens, Jason awakens on a school ...  \n",
      "2   Cugel is easily persuaded by the merchant Fia...  \n",
      "3   The book opens with Herald-Mage Vanyel return...  \n",
      "4   Taran and Gurgi have returned to Caer Dallben...  \n"
     ]
    }
   ],
   "source": [
    "# 1. 환경 설정 및 데이터 로드\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'book_reviews.csv'에는 컬럼 'review', 'genre'가 있습니다.\n",
    "df = pd.read_csv('book_reviews.csv')\n",
    "df['review'] = df['summary'] \n",
    "df.drop(columns=['summary'], inplace=True)\n",
    "print(df.shape)      # (샘플 수, 2)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d439f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 텍스트 전처리 및 토큰화\n",
    "import re\n",
    "def preprocess_text(text):\n",
    "    # TODO: text를 소문자 변환 후\n",
    "    text = text.lower()\n",
    "    # TODO: 특수문자 제거 (한글·영어·숫자 및 공백만 남기기)\n",
    "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
    "    # TODO: 띄어쓰기 기준 토큰화하여 리스트로 반환\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f77c55d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3. FastText 임베딩 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapi\u001b[39;00m\n\u001b[32m      3\u001b[39m fasttext = api.load(\u001b[33m\"\u001b[39m\u001b[33mfasttext-wiki-news-subwords-300\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m벡터 차원:\u001b[39m\u001b[33m\"\u001b[39m, fasttext.vector_size)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. FastText 임베딩 로드\n",
    "import gensim.downloader as api\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(\"벡터 차원:\", fasttext.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 단어 사전(vocab) 구축 및 시퀀스 변환\n",
    "from collections import Counter\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "all_tokens = [tok for tokens in df['tokens'] for tok in tokens]\n",
    "vocab_count = Counter(all_tokens)\n",
    "# TODO: 가장 빈도 높은 MAX_VOCAB_SIZE-1개의 단어를 뽑아 인덱스 매핑 생성\n",
    "# idx 0은 OOV용으로 예약\n",
    "\n",
    "def tokens_to_sequence(tokens, word2idx, maxlen=100):\n",
    "    # TODO: 각 토큰을 인덱스로 변환, OOV는 0\n",
    "    # TODO: 길이 > maxlen이면 자르고, < maxlen이면 0으로 패딩\n",
    "    return seq\n",
    "\n",
    "df['seq'] = df['tokens'].apply(lambda toks: tokens_to_sequence(toks, word2idx, maxlen=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970633b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 학습/검증 데이터 준비\n",
    "import numpy as np\n",
    "X = np.stack(df['seq'].values)              # (N, maxlen)\n",
    "y = df['genre'].factorize()[0]              # [0,1,2,...]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. PyTorch 모델 구현\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 6-1) 임베딩 레이어 초기화\n",
    "vocab_size = len(word2idx) + 1\n",
    "embed_dim = fasttext.vector_size\n",
    "\n",
    "embedding_matrix = torch.zeros(vocab_size, embed_dim)\n",
    "# TODO: word2idx에 해당하는 fasttext 벡터로 embedding_matrix 채우기\n",
    "# embedding_matrix[i] = fasttext[word] or zeros\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "# TODO: embedding_layer.weight.data에 embedding_matrix 할당\n",
    "# TODO: 임베딩 고정(freeze)할지 결정\n",
    "\n",
    "# 6-2) 분류기 클래스 정의\n",
    "class VanillaFC(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # TODO: 평균 임베딩 벡터 입력 → 예측 로직\n",
    "        pass\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # TODO: embedding_layer 복사\n",
    "        # TODO: nn.RNN 정의\n",
    "        # TODO: 마지막 시점 hidden → fc\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # TODO: nn.LSTM 정의\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # TODO: nn.GRU 정의\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 7. 학습 및 평가 함수\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train(model, optimizer, criterion, loader, device):\n",
    "    model.train()\n",
    "    # TODO: 배치 반복 → loss 계산 → 역전파 → optimizer.step()\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    # TODO: 배치 반복 → 예측 → 정확도 계산 → 평균 반환\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1da1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. 모델 학습 및 성능 비교\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = len(set(y))\n",
    "hidden_dim = 64\n",
    "\n",
    "models = {\n",
    "    'FC':   VanillaFC(input_dim=embed_dim, num_classes=num_classes),\n",
    "    'RNN':  RNNClassifier(embedding_layer, hidden_dim, num_classes),\n",
    "    'LSTM': LSTMClassifier(embedding_layer, hidden_dim, num_classes),\n",
    "    'GRU':  GRUClassifier(embedding_layer, hidden_dim, num_classes),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    # TODO: optimizer, criterion 설정\n",
    "    # TODO: train(model, ...) 호출\n",
    "    # TODO: val_acc = evaluate(model, ...) 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
