{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44ce00c",
   "metadata": {},
   "source": [
    "# ğŸš© ë¬¸ì œì§€ (Mission Brief)\n",
    "\n",
    "## ğŸ•ï¸ ìŠ¤í† ë¦¬: â€œìº í•‘ ë§ˆìŠ¤í„° AIâ€\n",
    "êµ­ë‚´ ìµœëŒ€ ì•„ì›ƒë„ì–´ ê¸°ì—… **ìº í•‘ ë§ˆìŠ¤í„°**ëŠ” â–³ì¥ë¹„ ì¶”ì²œ ì±—ë´‡ â–³ìº í•‘ì¥ ë¦¬ë·° ë¶„ì„ â–³ì‚¬ì „ ì•ˆì „ ì ê²€ ì•ŒëŒ ë“±ì„ ì œê³µí•˜ëŠ” **í†µí•© AI ì–´ì‹œìŠ¤í„´íŠ¸**ë¥¼ ì¶œì‹œí•˜ë ¤ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ë§‰ ì…ì‚¬í•œ **NLP ì—”ì§€ë‹ˆì–´**ë¡œì„œ, ë‹¤ìŒ ë„¤ ë‹¨ê³„ì˜ ëª¨ë“ˆì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬ì¶•â€§ì‹¤í—˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê° ë‹¨ê³„ì˜ **starter code**ëŠ” 50 % ì´ìƒ ì™„ì„±ë¼ ìˆìœ¼ë‹ˆ, `### TODO` í‘œì‹œ ë¶€ë¶„ë§Œ ì±„ì›Œì„œ ì‹¤í–‰Â·ê²€ì¦í•˜ì„¸ìš”.\n",
    "\n",
    "> **í™˜ê²½ ì¤€ë¹„** (í•„ìˆ˜)  \n",
    "> ```bash\n",
    "> pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "> pip install datasets transformers accelerate seqeval\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddac65",
   "metadata": {},
   "source": [
    "### ê³¼ì œ 1 (ì˜ë„ ë¶„ë¥˜ GRU)  \n",
    "**ëª©í‘œ** : ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ 150 ê°œ ì˜ë„ë¡œ ë¶„ë¥˜í•´ ì±—ë´‡ ë¼ìš°íŒ… ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.  \n",
    "**ë°ì´í„°** : Hugging Face â†’ `clinc_oos`, config `\"small\"`  \n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "raw = load_dataset(\"clinc_oos\", \"small\")\n",
    "print(raw[\"train\"][0])\n",
    "```\n",
    "#### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN, BATCH = 32, 64\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "pad_id = tok.pad_token_id\n",
    "\n",
    "def tokenize(b):\n",
    "    return tok(b[\"text\"], padding=\"max_length\",\n",
    "               truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "ds = load_dataset(\"clinc_oos\", \"small\")\n",
    "\n",
    "# ì»¬ëŸ¼ëª… intent â†’ labels\n",
    "ds = ds.rename_column(\"intent\", \"labels\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì¦ˆ\n",
    "ds = ds.map(tokenize, batched=True)\n",
    "\n",
    "# Torch í…ì„œ í˜•ì‹ìœ¼ë¡œ ì§€ì •\n",
    "ds.set_format(type=\"torch\",\n",
    "              columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_dl = DataLoader(ds[\"train\"], batch_size=BATCH, shuffle=True)\n",
    "test_dl  = DataLoader(ds[\"test\"],  batch_size=BATCH)\n",
    "\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim, hidden, num_labels):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.gru   = nn.GRU(embed_dim, hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc    = nn.Linear(hidden*2, num_labels)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.embed(ids)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.cat([x[:,0,:hidden], x[:,-1,hidden:]], dim=-1)  # ì–‘ë°©í–¥ ì²«Â·ë ìŠ¤í… ê²°í•©\n",
    "        return self.fc(x)\n",
    "\n",
    "### TODO 1-a : ì„ë² ë”© ì°¨ì›, hidden size ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„° ì§€ì •\n",
    "### TODO 1-b : í›ˆë ¨ ë£¨í”„(optimizer, loss ì •ì˜ ë° epoch ë°˜ë³µ) ì‘ì„±\n",
    "### TODO 1-c : í…ŒìŠ¤íŠ¸ ì…‹ ì •í™•ë„ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a988e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16e93b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ê³¼ì œ 2 (BiLSTM + CRF NE ì¸ì‹)  \n",
    "**ëª©í‘œ** : ìº í•‘ ì¤€ë¹„ë¬¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ì—ì„œ **ì¥ë¹„Â·ì§€ì—­Â·ë‚ ì§œ** ë“± ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•œë‹¤.  \n",
    "**ë°ì´í„°** : `conll2003` (ì˜ë¬¸) â€“ ì‹¤ì œ ìº í•‘ ë„ë©”ì¸ê³¼ ë‹¤ë¥´ì§€ë§Œ, ëª¨ë¸ ì•„í‚¤í…ì²˜ í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš©.  \n",
    "\n",
    "> CRF ê³„ì¸µ êµ¬í˜„ì´ ë¶€ë‹´ìŠ¤ëŸ¬ìš°ë©´, `seqeval` í‰ê°€ ì§€í‘œì™€ **BiLSTM+Linear** êµ¬ì¡°ë§Œ ì™„ì„±í•´ë„ í†µê³¼ì ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```python\n",
    "# ë°ì´í„° ì˜ˆì‹œ\n",
    "from datasets import load_dataset\n",
    "ner = load_dataset(\"conll2003\")\n",
    "print(ner[\"train\"][0])\n",
    "```\n",
    "#### Starter Code (BiLSTM Tagger skeleton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02804cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab, tagset, embed_dim=100, hidden=256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, embed_dim)\n",
    "        self.lstm  = nn.LSTM(embed_dim, hidden, num_layers=1,\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.fc    = nn.Linear(hidden*2, tagset)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        em = self.embed(ids)\n",
    "        out, _ = self.lstm(em)\n",
    "        return self.fc(out)          # [batch, seq, tag]\n",
    "\n",
    "### TODO 2-a : í† í¬ë‚˜ì´ì €ë¥¼ í›ˆë ¨ ì…‹ vocab ìœ¼ë¡œ í•™ìŠµ\n",
    "### TODO 2-b : padding & masking í•¨ìˆ˜ êµ¬í˜„\n",
    "### TODO 2-c : CrossEntropyLoss(ignore_index=pad_tag) ë¡œ ëª¨ë¸ í•™ìŠµ\n",
    "### TODO 2-d : seqeval (f1) ë¡œ ì„±ëŠ¥ ì¸¡ì •\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccac04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ê³¼ì œ 3 (1D CNN ë¦¬ë·° ê°ì„± ë¶„ë¥˜)  \n",
    "**ëª©í‘œ** : ìº í•‘ì¥ ë¦¬ë·°ë¥¼ ê¸ì •/ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ë³„ì  ì´ìƒ/ì´í•˜ë¥¼ íƒì§€í•œë‹¤.  \n",
    "**ë°ì´í„°** : Hugging Face â†’ `imdb` (binary sentiment)\n",
    "\n",
    "#### Starter Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6d634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim=128, num_classes=2, k=[3,4,5]):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab, embed_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(embed_dim, 100, kernel_size=ks) for ks in k])\n",
    "        self.fc = nn.Linear(100*len(k), num_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        x = self.embed(ids).transpose(1,2)                # [B,C,L]\n",
    "        x = torch.cat([torch.relu(c(x)).max(2)[0] for c in self.convs], 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "### TODO 3-a : í† í°í™”Â·DataLoader ì‘ì„±\n",
    "### TODO 3-b : í›ˆë ¨ ë£¨í”„ ë° ì •í™•ë„ í‰ê°€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d26e9",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìš”ì•½\n",
    "\n",
    "| ê³¼ì œ | ë°ì´í„°ì…‹ (ğŸ¤— Datasets) | í˜¸ì¶œ ì˜ˆì‹œ |\n",
    "|------|-----------------------|-----------|\n",
    "| 1 | `clinc_oos`, `\"small\"` | `load_dataset(\"clinc_oos\",\"small\")` |\n",
    "| 2 | `conll2003` | `load_dataset(\"conll2003\")` |\n",
    "| 3 | `imdb` | `load_dataset(\"imdb\")` |\n",
    "| 4 | `daily_dialog` | `load_dataset(\"daily_dialog\")` |\n",
    "\n",
    "### ë§ˆë¬´ë¦¬ íŒ\n",
    "- ê° ê³¼ì œ ìŠ¤í¬ë¦½íŠ¸ëŠ” `torch.save(model.state_dict(), \"...\")`ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•´ë‘ë©´, í›„ì† ì‹¤í—˜(í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì•™ìƒë¸”) ì‹œ ì‹œê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "- CPU ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤ìŠµí•˜ë ¤ë©´ `datasets` ì˜ `.select(range(N))`ë¡œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì—¬ ë””ë²„ê¹… í›„, GPU í›ˆë ¨ í™˜ê²½ì—ì„œ ì „ì²´ ë°ì´í„°ë¡œ ì¬í›ˆë ¨í•˜ì„¸ìš”.\n",
    "\n",
    "ì„±ê³µì ì¸ **ìº í•‘ ë§ˆìŠ¤í„° AI** ì¶œì‹œë¥¼ ê¸°ì›í•©ë‹ˆë‹¤! ğŸ•ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
